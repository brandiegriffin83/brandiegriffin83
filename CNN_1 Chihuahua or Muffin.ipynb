{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "GeHY84lwqnzO"
      },
      "source": [
        "# Chihuahua vs Muffin Classifier using Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  !git clone https://github.com/patitimoner/workshop-chihuahua-vs-muffin.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7kl6eYWq2_d",
        "outputId": "0a6e5e6a-5509-44cc-81a5-e0853c7326e6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'workshop-chihuahua-vs-muffin' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ37h-44qnzQ"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F70oMyptxOF",
        "outputId": "abaf743f-c4dc-4e0c-f346-3bc1929a31da"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "RFNxNWMYqnzQ"
      },
      "source": [
        "In this lab, we'll build upon our previous workshop where we used a traditional Neural Network (NN) to classify images as either Chihuahuas or muffins. This time, we'll use a Convolutional Neural Network (CNN), which is particularly well-suited for image classification tasks because it can learn spatial hierarchies of features directly from the image data.\n",
        "By the end of this lab, we'll compare the performance of our CNN model with the traditional NN from the previous workshop.\n",
        "\n",
        "This is what we'll do in this lab:\n",
        "#### 1) Build the  convolutional neural network\n",
        "#### 2) Load the data\n",
        "#### 3) Train the model on the data\n",
        "#### 4) Visualize the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWFCcQFrqnzQ"
      },
      "source": [
        "### Remember: This is an INTERACTIVE Notebook!\n",
        "You should run and play with the code as you go to see how it works. Select a cell and **press shift-enter to execute code.**\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "hKDqhxonqnzR"
      },
      "source": [
        "# 2.  Setup and Imports\n",
        "\n",
        "Let's get to the fun stuff!\n",
        "First, we need to Install and  import the necessary libraries. Each import serves a specific purpose in our project.\n",
        "python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-vDLPt_qnzR",
        "outputId": "ce23e54a-492c-4b89-b7ca-ef79af613a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.4.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch --upgrade\n",
        "!pip install torchvision --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIilnT4pqnzS",
        "outputId": "ef7e316b-9df4-4d85-efbf-10b611abd5cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np                          # Numpy for matrix operations\n",
        "import torch                                 # PyTorch deep learning framework\n",
        "import torch.nn as nn                        # Neural network module of PyTorch\n",
        "import torch.optim as optim                  # Optimization algorithms\n",
        "from torchvision import datasets, transforms # Tools for loading and transforming image data\n",
        "from torch.utils.data import DataLoader      # Efficient data loading\n",
        "import matplotlib.pyplot as plt              # For plotting and visualization. It is graphical library, to plot images\n",
        "# special Jupyter notebook command to show plots inline instead of in a new window\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm                        # For progress bars during training\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "uUbZRRemqnzS"
      },
      "source": [
        "# 3 Data Preparation\n",
        "Before we start training our model, it's crucial to separate our data into training and testing (validation) sets. This separation is a fundamental concept in machine learning that helps us assess how well our model generalizes to unseen data.\n",
        "\n",
        "## 3.1 Understanding Train-Test Split\n",
        "In machine learning, we typically divide our dataset into two main subsets:\n",
        "\n",
        "1. **Training set:** This is the larger portion of the data that we use to train our model. The model learns the patterns and features from this data.\n",
        "2. **Testing set** (also called Validation set): This is a smaller portion of the data that we set aside and don't use during training. We use this to evaluate how well our model performs on unseen data.\n",
        "\n",
        "The reason for this split is to simulate how our model would perform on new, unseen data in the real world. If we tested on the same data we used for training, we wouldn't know if our model was truly learning general patterns or just memorizing the training data (a problem called overfitting).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "UDu_U5LkqnzT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "6hC9YhJ5qnzT"
      },
      "source": [
        "\n",
        "## 3.2 Dataset Structure\n",
        "In our case, we've already separated our data into train and validation sets in our file structure:\n",
        "You should see that we have two main directories (same dataset as previous exercise): 'train' and 'validation', each containing subdirectories for our classes (Chihuahua and Muffin).\n",
        "\n",
        "## 3.3 Loading Separated Datasets\n",
        "Now, let's load our separated datasets:\n",
        "Remember we have to load all the images and convert them into a form that our neural network understands. Specifically, PyTorch works with **Tensor** objects. (A tensor is just a multidimensional matrix, i.e. an N-d array.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNsM3SpGqnzT"
      },
      "source": [
        "## 3.4 Define Data transformations\n",
        "Now that we understand our dataset, let's define the transformations we'll apply to our images. These transformations help in data augmentation and normalization.\n",
        "**To easily convert our image data into tensors, we use the help of a \"dataloader.\"** The dataloader packages data into convenient boxes for our model to use. You can think of it like one person passing boxes (tensors) to another.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rPYj3_plqnzT"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "#  Define image dimensions\n",
        "input_height, input_width = 224, 224\n",
        "\n",
        "# Define data transforms for training and validation sets\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((input_height, input_width)),  # Resize images\n",
        "        transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
        "        transforms.RandomRotation(10),  # Randomly rotate images\n",
        "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize((input_height, input_width)),  # Resize images\n",
        "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fqulJaRqnzU"
      },
      "source": [
        "## 3.5 Create Dataset and Dataloader\n",
        "With our transformations defined, we can now create our datasets and dataloaders. These will efficiently feed data into our model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZN6rbsKqnzU",
        "outputId": "c2269ef8-adf1-42c3-f6a9-0db2b355d680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 48000\n",
            "Validation set size: 12000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize the data\n",
        "])\n",
        "\n",
        "# Load the dataset (e.g., MNIST dataset)\n",
        "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets (80% train, 20% validation)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Check the size of dataset and dataloaders\n",
        "print(f'Training set size: {len(train_dataset)}')\n",
        "print(f'Validation set size: {len(val_dataset)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUYfeazZqnzU"
      },
      "source": [
        "### Let's break down what this code does:\n",
        "\n",
        "1. We define separate data transforms for training and validation sets. The training set includes data augmentation (random flips and rotations) to increase variety in our training data, while the validation set doesn't use augmentation.\n",
        "\n",
        "2. We use datasets.ImageFolder to load our images from the 'train' and 'validation' directories. This function automatically assigns labels based on the subdirectory names.\n",
        "\n",
        "3. We create DataLoader objects for both sets. These handle batching our data and shuffling the training set (but not the validation set, as order doesn't matter for validation).\n",
        "\n",
        "4. Finally, we print the sizes of our datasets to confirm the split.\n",
        "\n",
        "By using separate dataloaders for training and validation, we ensure that our model is evaluated on data it hasn't seen during training, giving us a more accurate assessment of its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UYzq8xfqnzU"
      },
      "source": [
        "# 4. Model Definition\n",
        "Now that we've prepared our data, we can define our CNN model. We'll use the information about our input dimensions and number of classes to structure our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7LYsAJ3qnzU",
        "outputId": "d123dd78-5b8a-4afd-be99-0ae1543c810c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChihuahuaMuffinCNN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=8192, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 64, 64]             896\n",
            "              ReLU-2           [-1, 32, 64, 64]               0\n",
            "         MaxPool2d-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          18,496\n",
            "              ReLU-5           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-6           [-1, 64, 16, 16]               0\n",
            "            Conv2d-7          [-1, 128, 16, 16]          73,856\n",
            "              ReLU-8          [-1, 128, 16, 16]               0\n",
            "         MaxPool2d-9            [-1, 128, 8, 8]               0\n",
            "           Linear-10                  [-1, 512]       4,194,816\n",
            "             ReLU-11                  [-1, 512]               0\n",
            "          Dropout-12                  [-1, 512]               0\n",
            "           Linear-13                    [-1, 2]           1,026\n",
            "================================================================\n",
            "Total params: 4,289,090\n",
            "Trainable params: 4,289,090\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 3.95\n",
            "Params size (MB): 16.36\n",
            "Estimated Total Size (MB): 20.36\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ChihuahuaMuffinCNN(nn.Module):\n",
        "    def __init__(self, input_height, input_width, input_channels=3, num_classes=2):\n",
        "        super(ChihuahuaMuffinCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.features = nn.Sequential(\n",
        "            # First convolutional layer\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Second convolutional layer\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Third convolutional layer\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Calculate the size of the flattened feature map after convolution and pooling\n",
        "        self.flattened_size = 128 * (input_height // 8) * (input_width // 8)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.flattened_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # Pass input through convolutional layers\n",
        "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layers\n",
        "        x = self.classifier(x)  # Pass through the fully connected layers\n",
        "        return x\n",
        "\n",
        "# Example dimensions (use appropriate height and width based on your dataset)\n",
        "input_height = 64  # Adjust according to your dataset\n",
        "input_width = 64   # Adjust according to your dataset\n",
        "num_classes = 2    # Chihuahua or Muffin (Binary classification)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model and move it to the appropriate device\n",
        "model = ChihuahuaMuffinCNN(input_height, input_width, num_classes=num_classes).to(device)\n",
        "print(model)\n",
        "\n",
        "# Print model summary (requires torchsummary)\n",
        "from torchsummary import summary\n",
        "summary(model, (3, input_height, input_width))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiUAhQQKqnzU"
      },
      "source": [
        "# 5. Training Setup\n",
        "With our model defined, we now need to set up our loss function and optimizer. These are crucial components for training our network. In deep learning, they are the  two key components for training: Let's  recap and understand what these are and how we'll use them.\n",
        "\n",
        "**Loss Function**\n",
        "The loss function measures how well our model is performing. It calculates the difference between our model's predictions and the true labels. For our classification task, we'll use Cross Entropy Loss, which is well-suited for multi-class classification problems.\n",
        "\n",
        "**Optimizer**\n",
        "The optimizer is responsible for updating our model's parameters to minimize the loss function. We'll use the Adam optimizer, which is an extension of stochastic gradient descent (SGD) that adapts the learning rate for each parameter.\n",
        "\n",
        "Let's define our loss function and optimizer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0fKBAewqnzV",
        "outputId": "f724d673-8375-4d89-c4aa-38cab780b0e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function: CrossEntropyLoss()\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function (Cross Entropy for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer (Adam) with learning rate 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Loss function:\", criterion)\n",
        "print(\"Optimizer:\", optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4kvUGAxqnzV"
      },
      "source": [
        "### In this code:\n",
        "\n",
        "nn.CrossEntropyLoss() creates our loss function.\n",
        "optim.Adam(model.parameters(), lr=0.001) creates our optimizer. We pass it our model's parameters and set a learning rate of 0.001."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsMe4-iFqnzV"
      },
      "source": [
        "# 6. Model Trainning\n",
        "Now we're ready to train our model. We'll define a function to handle the training process and then run it for a specified number of epochs.\n",
        "\n",
        "This function will:\n",
        "- Iterate over our data for a specified number of epochs\n",
        "\n",
        "In each epoch, it will:\n",
        "- Train on the training data\n",
        "- Evaluate on the validation data\n",
        "\n",
        "Keep track of and print our loss and accuracy for both training and validation sets\n",
        "This will start the training process. You'll see progress bars for each epoch, along with loss and accuracy metrics for both training and validation sets.\n",
        "\n",
        "The training process may take some time, depending on your hardware. Once it's complete, we'll have a trained model ready for making predictions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "AaEqGJiGqnzV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in tqdm(dataloaders[phase], desc=phase):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward pass + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Calculate loss and accuracy\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        print()\n",
        "\n",
        "    return model\n",
        "\n",
        "dataloaders = {\n",
        "    'train': train_loader,  # DataLoader for training data\n",
        "    'validation': val_loader  # DataLoader for validation data\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkLBbL9UqnzV"
      },
      "source": [
        " ###This function does the following:\n",
        "\n",
        "1. For each epoch:\n",
        "-  It trains the model on the training data.\n",
        "-  It then evaluates the model on the validation data.\n",
        "-  For both phases, it calculates and prints the average loss and accuracy.\n",
        "\n",
        "\n",
        "2. The model.train() and model.eval() calls ensure the model behaves appropriately for training and validation phases.\n",
        "\n",
        "3. We use torch.set_grad_enabled() to only calculate gradients during the training phase.\n",
        "\n",
        "4. In the training phase, we perform backpropagation (loss.backward()) and update the model parameters (optimizer.step())."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "1I__P3CDqnzV"
      },
      "source": [
        "# 7. Examine model performance (Model Evaluation)\n",
        "Finally, let's evaluate our trained model on the validation set and visualize some of its predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "kBK-QA_XqnzV"
      },
      "outputs": [],
      "source": [
        "# In your model definition (e.g., in the `__init__` of your ChihuahuaMuffinCNN class):\n",
        "class ChihuahuaMuffinCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChihuahuaMuffinCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1), # Change in_channels to 1\n",
        "            # ... (rest of your model definition) ...\n",
        "        )\n",
        "        # ... (rest of your model definition) ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSYovJLZqnzW"
      },
      "source": [
        "# 8.  Conclusion and Reflection\n",
        "\n",
        "Congratulations! You've successfully built, trained, and evaluated a CNN for classifying Chihuahuas and Muffins. Here are some reflection questions to consider:\n",
        "\n",
        "How does the performance of this CNN compare to the traditional Neural Network from the previous workshop?\n",
        "What role do the convolutional layers play in image classification?\n",
        "How might you further improve this model's performance?\n",
        "What challenges might this model face in real-world applications?\n",
        "How does data augmentation (like random flips and rotations) contribute to the model's performance?\n",
        "What are the ethical considerations in developing and deploying an image classification system like this?\n",
        "\n",
        "Remember to support your answers with references to relevant literature or resources on deep learning and computer vision. Good luck with your reflective journal!\n",
        "\n",
        " If you want  you can play with some hyperparameters to play with:\n",
        "- Number of epochs\n",
        "- The learning rate \"lr\" parameter in the optimizer\n",
        "- The type of optimizer (https://pytorch.org/docs/stable/optim.html)\n",
        "- Number of layers and layer dimensions\n",
        "- Image size\n",
        "- Data augmentation transforms (https://pytorch.org/docs/stable/torchvision/transforms.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "zDtxawYLqnzW"
      },
      "source": [
        "# Special Thanks!\n",
        "\n",
        "Credit for the original idea and code goes to [DeepSense.ai](https://deepsense.ai/keras-vs-pytorch-avp-transfer-learning/)!\n",
        "I've modified it significantly to cater to this Lab.\n",
        "\n",
        "The original tutorial was created through hard work and love by Jing Zhao, Dylan Wang, Jason Do, Jason Jiang, and Andrew Jong."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}